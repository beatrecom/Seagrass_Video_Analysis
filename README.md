Seeing the Seafloor Smarter: Using AI to Classify Seagrass from ASV Video
Expanded Summary:
As part of a COMIT-supported research effort, Bea Combs-Hintze has developed an image recognition system and training database that turns hours of underwater video footage from autonomous surface vessels (ASVs) into searchable, structured seagrass data.
This tool is designed to identify and classify key habitat features—such as seagrass presence, species type, density, and structural complexity—by processing still frames extracted from ASV video.
What makes this especially powerful is that the video footage is georeferenced and time-synced with acoustic sonar data (like water column or backscatter from multibeam echosounders). That means researchers can now visually confirm or train models to predict what different seagrass structures look like on sonar—a major leap toward automated, scalable seagrass mapping.
 
🧠 How It Works (Simple Steps)
1.	ASV surveys collect both sonar and HD video in shallow coastal areas
2.	Video is broken into frames, then annotated using seagrass ID guides
3.	Labeled images are stored in a growing database of known visual features
4.	These images train a machine learning model to recognize patterns: Is it seagrass? What species? How dense?
5.	The result: a system that helps scientists rapidly process video and link it directly to acoustic signals.
 
🌱 Why This Matters
•	🧭 Faster: Reduces the need for frame-by-frame manual video review
•	🧠 Smarter: Trains sonar models with real-world visuals, improving accuracy
•	🔁 Scalable: Enables broader habitat classification efforts with AI
•	🎓 Educational: Used as a training tool for interns and early-career scientists
 
💡 Where It’s Headed
This system is being tested as a companion to seagrass acoustic classification, meaning you could one day use sonar alone to map seagrass structure—because your AI already knows what “dense Thalassia canopy” looks like in both video and sonar form.

